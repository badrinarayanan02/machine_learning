{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpDmhvj4s9chi08wOuAGx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrinarayanan02/machine_learning/blob/main/2348507_NLPlab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "1ccf1gKtLRvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of converting a large texts into smaller parts called as tokens. This will help us to find new patterns. It is considered as a base step for stemming and lemmatization."
      ],
      "metadata": {
        "id": "iA_eEp6lPD_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write a paragraph based on your interested Domain and that should incorporate the  special characters, punctuations, stop words, negation (donâ€™t) and emojis (try to add  more than one emoji contiguously in sentence). Perform the following types  of Tokenization and utilize the Python libraries to tokenize."
      ],
      "metadata": {
        "id": "OrwSZVx6LQRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMOVVWwtQcwC",
        "outputId": "117b1e9e-c94c-4c1a-af5c-7c6cb4055573"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence"
      ],
      "metadata": {
        "id": "071EfHHHOCdX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EPXxcv12LCxT"
      },
      "outputs": [],
      "source": [
        "data = \"In the vast realm of astrophysics, researchers delve into the mysteries of the cosmos, examining celestial bodies and their enigmatic behaviors. The interstellar medium, filled with intricate complexities, presents challenges that astronomers and physicists don't take lightly. Exploring the universe requires advanced instrumentation and cutting-edge technology. However, the quest for understanding doesn't stop there â€“ it extends beyond the confines of our own galaxy. Don't underestimate the significance of cosmic phenomena; they hold the key to unraveling the secrets of existence. ðŸŒŒâœ¨ The relentless pursuit of knowledge fuels the curiosity of scientists, propelling humanity toward a greater comprehension of the cosmos. ðŸš€ðŸ”­ \""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenizer"
      ],
      "metadata": {
        "id": "FacwJhC5b61z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euxhrFZKP2i1",
        "outputId": "09a1635c-1464-48af-923e-1103281c65aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', ',', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', ',', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', '.', 'The', 'interstellar', 'medium', ',', 'filled', 'with', 'intricate', 'complexities', ',', 'presents', 'challenges', 'that', 'astronomers', 'and', 'physicists', 'do', \"n't\", 'take', 'lightly', '.', 'Exploring', 'the', 'universe', 'requires', 'advanced', 'instrumentation', 'and', 'cutting-edge', 'technology', '.', 'However', ',', 'the', 'quest', 'for', 'understanding', 'does', \"n't\", 'stop', 'there', 'â€“', 'it', 'extends', 'beyond', 'the', 'confines', 'of', 'our', 'own', 'galaxy', '.', 'Do', \"n't\", 'underestimate', 'the', 'significance', 'of', 'cosmic', 'phenomena', ';', 'they', 'hold', 'the', 'key', 'to', 'unraveling', 'the', 'secrets', 'of', 'existence', '.', 'ðŸŒŒâœ¨', 'The', 'relentless', 'pursuit', 'of', 'knowledge', 'fuels', 'the', 'curiosity', 'of', 'scientists', ',', 'propelling', 'humanity', 'toward', 'a', 'greater', 'comprehension', 'of', 'the', 'cosmos', '.', 'ðŸš€ðŸ”­']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The outcome of this tokenizer is it's splits the data into words."
      ],
      "metadata": {
        "id": "hiKl_qnqSaxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenizer"
      ],
      "metadata": {
        "id": "mEtASpFHb9m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_8a11rAQvV0",
        "outputId": "691026e9-1afd-43e7-9b8e-c2e05be5a57c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In the vast realm of astrophysics, researchers delve into the mysteries of the cosmos, examining celestial bodies and their enigmatic behaviors.', \"The interstellar medium, filled with intricate complexities, presents challenges that astronomers and physicists don't take lightly.\", 'Exploring the universe requires advanced instrumentation and cutting-edge technology.', \"However, the quest for understanding doesn't stop there â€“ it extends beyond the confines of our own galaxy.\", \"Don't underestimate the significance of cosmic phenomena; they hold the key to unraveling the secrets of existence.\", 'ðŸŒŒâœ¨ The relentless pursuit of knowledge fuels the curiosity of scientists, propelling humanity toward a greater comprehension of the cosmos.', 'ðŸš€ðŸ”­']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This tokenizer splits the data into sentences."
      ],
      "metadata": {
        "id": "CEI3iFxmR9BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation Tokenizer"
      ],
      "metadata": {
        "id": "7IRWRj6WcDHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl2VEWCuRt2e",
        "outputId": "58a75f09-fe50-4fff-ff77-ea4d5f051983"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', ',', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', ',', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', '.', 'The', 'interstellar', 'medium', ',', 'filled', 'with', 'intricate', 'complexities', ',', 'presents', 'challenges', 'that', 'astronomers', 'and', 'physicists', 'don', \"'\", 't', 'take', 'lightly', '.', 'Exploring', 'the', 'universe', 'requires', 'advanced', 'instrumentation', 'and', 'cutting', '-', 'edge', 'technology', '.', 'However', ',', 'the', 'quest', 'for', 'understanding', 'doesn', \"'\", 't', 'stop', 'there', 'â€“', 'it', 'extends', 'beyond', 'the', 'confines', 'of', 'our', 'own', 'galaxy', '.', 'Don', \"'\", 't', 'underestimate', 'the', 'significance', 'of', 'cosmic', 'phenomena', ';', 'they', 'hold', 'the', 'key', 'to', 'unraveling', 'the', 'secrets', 'of', 'existence', '.', 'ðŸŒŒâœ¨', 'The', 'relentless', 'pursuit', 'of', 'knowledge', 'fuels', 'the', 'curiosity', 'of', 'scientists', ',', 'propelling', 'humanity', 'toward', 'a', 'greater', 'comprehension', 'of', 'the', 'cosmos', '.', 'ðŸš€ðŸ”­']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This tokenizer splits the sentences into words based on punctuations."
      ],
      "metadata": {
        "id": "--JbuMQ4R1LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TreebankWord Tokenizer"
      ],
      "metadata": {
        "id": "Ri1IYMWgcSGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEOvP5byTC02",
        "outputId": "6363da55-2020-4af2-adc9-139514fb7e2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', ',', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', ',', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors.', 'The', 'interstellar', 'medium', ',', 'filled', 'with', 'intricate', 'complexities', ',', 'presents', 'challenges', 'that', 'astronomers', 'and', 'physicists', 'do', \"n't\", 'take', 'lightly.', 'Exploring', 'the', 'universe', 'requires', 'advanced', 'instrumentation', 'and', 'cutting-edge', 'technology.', 'However', ',', 'the', 'quest', 'for', 'understanding', 'does', \"n't\", 'stop', 'there', 'â€“', 'it', 'extends', 'beyond', 'the', 'confines', 'of', 'our', 'own', 'galaxy.', 'Do', \"n't\", 'underestimate', 'the', 'significance', 'of', 'cosmic', 'phenomena', ';', 'they', 'hold', 'the', 'key', 'to', 'unraveling', 'the', 'secrets', 'of', 'existence.', 'ðŸŒŒâœ¨', 'The', 'relentless', 'pursuit', 'of', 'knowledge', 'fuels', 'the', 'curiosity', 'of', 'scientists', ',', 'propelling', 'humanity', 'toward', 'a', 'greater', 'comprehension', 'of', 'the', 'cosmos.', 'ðŸš€ðŸ”­']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This TreeBankWordTokenizer removes the phrase terminating punctuations like (?,') and retains decimal numbers as single token.\n",
        "\n",
        "For Example in the data given, for the word don't it splitted like \"do\" \"n't\""
      ],
      "metadata": {
        "id": "DEO65_tpTrMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet Tokenizer"
      ],
      "metadata": {
        "id": "9XnC-1UkcWVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ4LNMw4VlvN",
        "outputId": "61d677a1-1bd1-4ce4-84e0-55abb2054619"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', ',', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', ',', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', '.', 'The', 'interstellar', 'medium', ',', 'filled', 'with', 'intricate', 'complexities', ',', 'presents', 'challenges', 'that', 'astronomers', 'and', 'physicists', \"don't\", 'take', 'lightly', '.', 'Exploring', 'the', 'universe', 'requires', 'advanced', 'instrumentation', 'and', 'cutting-edge', 'technology', '.', 'However', ',', 'the', 'quest', 'for', 'understanding', \"doesn't\", 'stop', 'there', 'â€“', 'it', 'extends', 'beyond', 'the', 'confines', 'of', 'our', 'own', 'galaxy', '.', \"Don't\", 'underestimate', 'the', 'significance', 'of', 'cosmic', 'phenomena', ';', 'they', 'hold', 'the', 'key', 'to', 'unraveling', 'the', 'secrets', 'of', 'existence', '.', 'ðŸŒŒ', 'âœ¨', 'The', 'relentless', 'pursuit', 'of', 'knowledge', 'fuels', 'the', 'curiosity', 'of', 'scientists', ',', 'propelling', 'humanity', 'toward', 'a', 'greater', 'comprehension', 'of', 'the', 'cosmos', '.', 'ðŸš€', 'ðŸ”­']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** It is used to split the emoji's into different words, it can be a base for performing sentimental analysis"
      ],
      "metadata": {
        "id": "1WkcwlzoWo6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiword Expression Tokenizer"
      ],
      "metadata": {
        "id": "_q3tfWVgcg3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"In the vast realm of astrophysics, researchers delve into the mysteries of the cosmos, examining celestial bodies and their enigmatic behaviors in astro space\"\n",
        "tokenizer = MWETokenizer()\n",
        "print(tokenizer.tokenize(word_tokenize(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMyf12XIXNeB",
        "outputId": "657572b6-27c9-43d4-f598-957cf38e9c68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', ',', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', ',', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', 'in', 'astro', 'space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"In the vast realm of astrophysics, researchers delve into the mysteries of the cosmos, examining celestial bodies and their enigmatic behaviors in astro space\"\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('astro','space'))\n",
        "print(tokenizer.tokenize(word_tokenize(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu2PghiyaTqD",
        "outputId": "db4efd7f-1e2e-40ae-c1e9-362870da3df6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', ',', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', ',', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', 'in', 'astro_space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Multi Word Expression Tokenizer allows to combine multiple word expressions, it can merge multi word into single tokens"
      ],
      "metadata": {
        "id": "Xe8ExkN-WpAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textblob Tokenizer"
      ],
      "metadata": {
        "id": "DuoAfNh1cl3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(data)\n",
        "words = blob.words\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGVb9dBsbGND",
        "outputId": "c7bd7624-bca1-4e61-8fa3-c18edd511c71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', 'in', 'astro', 'space']\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** TextBlob is another tokenizer, through the output we can analyse that it will remove the punctuation marks from the data."
      ],
      "metadata": {
        "id": "FHbS2Yz3bkjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy Tokenizer"
      ],
      "metadata": {
        "id": "P-lIQJczc0s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = spacy.blank('en')\n",
        "doc = var(data)\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBySnCktbkTh",
        "outputId": "1af7336d-eef3-46eb-89b3-39f955257b86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In\n",
            "the\n",
            "vast\n",
            "realm\n",
            "of\n",
            "astrophysics\n",
            ",\n",
            "researchers\n",
            "delve\n",
            "into\n",
            "the\n",
            "mysteries\n",
            "of\n",
            "the\n",
            "cosmos\n",
            ",\n",
            "examining\n",
            "celestial\n",
            "bodies\n",
            "and\n",
            "their\n",
            "enigmatic\n",
            "behaviors\n",
            "in\n",
            "astro\n",
            "space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Spacy tokenizer provides flexibility to specify special tokens."
      ],
      "metadata": {
        "id": "5k5Ws9J4djQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim Word Tokenizer"
      ],
      "metadata": {
        "id": "Jo9lh2pDc1nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gensimTokenizer = list(tokenize(data))\n",
        "print(gensimTokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m7TBDTceZLn",
        "outputId": "5f4869da-1953-4c79-9eb1-bb3780b3149d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'vast', 'realm', 'of', 'astrophysics', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', 'in', 'astro', 'space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization with Keras"
      ],
      "metadata": {
        "id": "hn_9so8lffV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notoken = Tokenizer(num_words = 30)\n",
        "notoken.fit_on_texts(data)\n",
        "listOfWords = text_to_word_sequence(data)\n",
        "print(listOfWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP2vzYmVfju-",
        "outputId": "67910377-9f2d-4ee0-d953-d6234a627012"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in', 'the', 'vast', 'realm', 'of', 'astrophysics', 'researchers', 'delve', 'into', 'the', 'mysteries', 'of', 'the', 'cosmos', 'examining', 'celestial', 'bodies', 'and', 'their', 'enigmatic', 'behaviors', 'in', 'astro', 'space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The advantage of keras tokenizer, is it will convert the data into lower case before tokenizing it."
      ],
      "metadata": {
        "id": "ZPfo8niogxMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "The motive behind this exercise is to experiment different tokenizer by using existing libraries, since this tokenizing is a base for stemming and lemmatization tasks in nlp."
      ],
      "metadata": {
        "id": "iz9TJpcJg_v5"
      }
    }
  ]
}