{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrinarayanan02/machine_learning/blob/main/2348507_AMLlab10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning**"
      ],
      "metadata": {
        "id": "BQdDsaMVzdl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python script for implementing Q-Learning menthod.\n",
        "Q learning is a value-based method of supplying information to inform which action an agent should\n",
        "take."
      ],
      "metadata": {
        "id": "UVAIC_Nqz7fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are five rooms in a building which are connected by doors.\n",
        "\n",
        "Each room is numbered 0 to 4,\n",
        "The outside of the building can be one big outside area(5),\n",
        "Doors number 1 and 4 lead into the building from room 5,\n",
        "Display / print initial matrix understanding the layout given below.\n",
        "Implement Q-Learning method for generating all possible states and actions. Display / print final\n",
        "Convergence Value matrix."
      ],
      "metadata": {
        "id": "AAgfWzGI08-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "2WDzvP34251j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7bB7UfyzPGD"
      },
      "outputs": [],
      "source": [
        "num_rooms = 5\n",
        "\n",
        "num_actions = 4\n",
        "\n",
        "layout = [\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [0, 0, 0, 1, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 1],\n",
        "    [1, 0, 0, 0, 0],\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q learning paramaters"
      ],
      "metadata": {
        "id": "_GjtGncy9NR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "# Initialize Q matrix\n",
        "Q = np.zeros((num_rooms, num_actions))"
      ],
      "metadata": {
        "id": "iOqRHaqs9Mpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Available Actions"
      ],
      "metadata": {
        "id": "Omq6kEZh3V8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_available_actions(state):\n",
        "    return [action for action in range(num_actions) if layout[state][action] == 1]"
      ],
      "metadata": {
        "id": "RgRygBUU3IpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q Learning Algorithm"
      ],
      "metadata": {
        "id": "4jmLXU8l3ekm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning():\n",
        "    num_episodes = 1000\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        current_state = np.random.randint(num_rooms)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            available_actions = get_available_actions(current_state)\n",
        "\n",
        "            if not available_actions:\n",
        "                break\n",
        "\n",
        "            if np.random.uniform(0, 1) < epsilon:\n",
        "                action = np.random.choice(available_actions)\n",
        "            else:\n",
        "                action = np.argmax(Q[current_state])\n",
        "\n",
        "            next_state = np.argmax(layout[current_state])\n",
        "            reward = 1 if next_state == num_rooms - 1 else 0  # Reward 1 if outside, else 0\n",
        "\n",
        "\n",
        "            Q[current_state, action] = (1 - alpha) * Q[current_state, action] + alpha * (reward + gamma * np.max(Q[next_state]))\n",
        "\n",
        "            current_state = next_state\n",
        "            done = (current_state == num_rooms - 1)\n",
        "\n",
        "\n",
        "print(\"Initial Layout Matrix:\")\n",
        "print(np.array(layout))\n",
        "\n",
        "q_learning()\n",
        "\n",
        "print(\"\\nFinal Q Matrix (Convergence Value Matrix):\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbuBWAEY3doO",
        "outputId": "b9b3f978-5ed1-4ebc-9a0d-52a406c1d9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Layout Matrix:\n",
            "[[0 1 0 0 0]\n",
            " [0 0 0 1 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 1]\n",
            " [1 0 0 0 0]]\n",
            "\n",
            "Final Q Matrix (Convergence Value Matrix):\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This indicates that after training, the Q values for all state-action pairs have converged to zero. This might be due to the limited exploration or insufficient training episodes.\n"
      ],
      "metadata": {
        "id": "LRSHxUZ97cVd"
      }
    }
  ]
}