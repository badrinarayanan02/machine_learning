{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdHMymBY9vQUBso83lJKnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrinarayanan02/machine_learning/blob/main/2348507_NLPlab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Different Operations**\n"
      ],
      "metadata": {
        "id": "SLgnT3J23yoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krmY4mCv3gZ7",
        "outputId": "5a9beec6-024f-4f2f-d6c0-5f31a61885ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer,WordNetLemmatizer\n",
        "from nltk.stem.snowball import GermanStemmer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import RegexpParser\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Program to get Antonyms from WordNet"
      ],
      "metadata": {
        "id": "yhlzrTa53l-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSynonymsAntonyms(word):\n",
        "    synonyms = []\n",
        "    antonyms = []\n",
        "\n",
        "    synsets = wordnet.synsets(word)\n",
        "    # Iterating over the synsets\n",
        "    for synset in synsets:\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "            # checking if the lemma has antonyms\n",
        "            if lemma.antonyms():\n",
        "                antonyms.append(lemma.antonyms()[0].name())\n",
        "    return synonyms, antonyms\n",
        "\n",
        "print(\"Synonyms and Antonyms of the given words\")\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "words = [\"happy\",\"good\", \"selfish\", \"cloudy\", \"true\"]\n",
        "for word in words:\n",
        "    synonyms, antonyms = getSynonymsAntonyms(word)\n",
        "    print(\"Synonyms for\", word, \":\", synonyms)\n",
        "    print(\"Antonyms for\", word, \":\", antonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOtZaI5z4mZp",
        "outputId": "8c22bc0d-0fdd-40ef-a36c-73f401f4198b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms and Antonyms of the given words\n",
            "----------------------------------------\n",
            "Synonyms for happy : ['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']\n",
            "Antonyms for happy : ['unhappy']\n",
            "Synonyms for good : ['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good']\n",
            "Antonyms for good : ['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n",
            "Synonyms for selfish : ['selfish']\n",
            "Antonyms for selfish : ['unselfish']\n",
            "Synonyms for cloudy : ['cloudy', 'nebulose', 'nebulous', 'cloudy', 'cloudy', 'muddy', 'mirky', 'murky', 'turbid']\n",
            "Antonyms for cloudy : ['clear']\n",
            "Synonyms for true : ['true', 'true', 'true_up', 'true', 'true', 'dead_on_target', 'true', 'truthful', 'true', 'true', 'dependable', 'honest', 'reliable', 'true', 'genuine', 'true', 'unfeigned', 'true', 'true', 'true', 'lawful', 'rightful', 'on-key', 'true', 'true', 'straight', 'true', 'admittedly', 'avowedly', 'confessedly']\n",
            "Antonyms for true : ['false', 'untruthful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Thus the program to get the antonyms from WordNet has been done successfully."
      ],
      "metadata": {
        "id": "9wDQhg0E6d92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Program for stemming Non-English words"
      ],
      "metadata": {
        "id": "Q45f088I6riU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "germanSt = GermanStemmer()"
      ],
      "metadata": {
        "id": "ep7VUFUR49cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = [\"Danke\",\"geschrieben\",\"Kuchen\",\"katze\"]"
      ],
      "metadata": {
        "id": "2graKQZt7NuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemWords = [germanSt.stem(words) for words in token]\n",
        "print(stemWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYIlYUr67Q5b",
        "outputId": "b06b6f40-41b6-471c-d4a7-9a0e9f7f1cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dank', 'geschrieb', 'kuch', 'katz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The german words are tokenized. Meanings -> (danke - Thankyou, geschrieben - written, kuchen - Cake, katze - Cat)."
      ],
      "metadata": {
        "id": "hiy9va_m7yPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Program for lemmatizing words using WordNet"
      ],
      "metadata": {
        "id": "kuU7QaqB8RRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porterStemmer = PorterStemmer()\n",
        "lancasterStemmer = LancasterStemmer()\n",
        "snowballStemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# function to stem a word\n",
        "def stem_word(word):\n",
        "    porter_stemmed_word = porterStemmer.stem(word)\n",
        "    lancaster_stemmed_word = lancasterStemmer.stem(word)\n",
        "    snowball_stemmed_word = snowballStemmer.stem(word)\n",
        "\n",
        "    print(word, \"-> Porter:\", porter_stemmed_word,\n",
        "          \"-> Lancaster:\", lancaster_stemmed_word,\n",
        "          \"-> Snowball:\", snowball_stemmed_word)\n",
        "\n",
        "words = [\"cats\", \"dogs\", \"shipping\", \"chair\", \"man\"]\n",
        "\n",
        "for word in words:\n",
        "    stem_word(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gRjIbkQD0nc",
        "outputId": "6a9d0e32-6eae-4603-8bb1-2566b5897769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats -> Porter: cat -> Lancaster: cat -> Snowball: cat\n",
            "dogs -> Porter: dog -> Lancaster: dog -> Snowball: dog\n",
            "shipping -> Porter: ship -> Lancaster: ship -> Snowball: ship\n",
            "chair -> Porter: chair -> Lancaster: chair -> Snowball: chair\n",
            "man -> Porter: man -> Lancaster: man -> Snowball: man\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Thus the given words in the list has been lemmatized by using different lemmatizing techniques."
      ],
      "metadata": {
        "id": "qrvaePQbRmnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Program to differentiate stemming and lemmatizing words\n"
      ],
      "metadata": {
        "id": "hnmT3xRlOQl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Running ducks and swimming geese are better than flying birds\"\n",
        "\n",
        "words = word_tokenize(input_text)\n",
        "\n",
        "# stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "\n",
        "# lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "RtZB47LoOU5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d874281-15b8-4eb2-bce7-848581e9cd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['run', 'duck', 'and', 'swim', 'gees', 'are', 'better', 'than', 'fli', 'bird']\n",
            "Lemmatized words: ['Running', 'duck', 'and', 'swim', 'geese', 'be', 'better', 'than', 'fly', 'bird']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Here we can interpret the differences between stemming and lemmatizing"
      ],
      "metadata": {
        "id": "XBT2HaW3S7qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Program for PoS Tagging"
      ],
      "metadata": {
        "id": "EnVhCAuy_NXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"My Spirituality guru is Gaur Gopal Das\"\n",
        "\n",
        "words = word_tokenize(input_text)\n",
        "\n",
        "# applying POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_IpB_Xs8YqB",
        "outputId": "e6f1ffe7-53a4-4333-d57d-b23b21c0097d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('My', 'PRP$'), ('Spirituality', 'NNP'), ('guru', 'NN'), ('is', 'VBZ'), ('Gaur', 'NNP'), ('Gopal', 'NNP'), ('Das', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The part of speech tagging is implemented for the given corpus"
      ],
      "metadata": {
        "id": "ozJYZI2YWW_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Program to Identify Named Entity Recognition"
      ],
      "metadata": {
        "id": "70K_FZ9E8YeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\""
      ],
      "metadata": {
        "id": "ztZmD3K3KwFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(corpus)\n",
        "\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "for entity in named_entities:\n",
        "    if hasattr(entity, 'label'):\n",
        "        print(' '.join(c[0] for c in entity), '=>', entity.label())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67KyfhoQKY3e",
        "outputId": "c7614633-dfcd-4170-a817-173539b5fc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack => PERSON\n",
            "Obama => PERSON\n",
            "United States => GPE\n",
            "Hawaii => GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** For the given corpus the named entity recognition is obtained. (Barack, Obama, -> Person), (United States, Hawaii -> Geographical Entity)."
      ],
      "metadata": {
        "id": "WXDTehCNUbQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Implementing Dependency Parsing and Constituency Parsing"
      ],
      "metadata": {
        "id": "bGh1FTf-NRgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "doc = nlp(sentence)\n",
        "print(\"\\nDependency Parsing with spaCy:\")\n",
        "for token in doc:\n",
        "    print(token.text, \"-->\", token.dep_, \"-->\", token.head.text)\n",
        "\n",
        "print(\"\\nConstituency Parsing with NLTK:\")\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "grammar = r\"\"\"\n",
        "    NP: {<DT|JJ|NN.*>+}\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "parsed_sentence = chunk_parser.parse(pos_tags)\n",
        "print(parsed_sentence)"
      ],
      "metadata": {
        "id": "slN8Aqt2LaCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5fa4fc-9997-412e-de28-49e77d94ed7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dependency Parsing with spaCy:\n",
            "The --> det --> fox\n",
            "quick --> amod --> fox\n",
            "brown --> amod --> fox\n",
            "fox --> nsubj --> jumps\n",
            "jumps --> ROOT --> jumps\n",
            "over --> prep --> jumps\n",
            "the --> det --> dog\n",
            "lazy --> amod --> dog\n",
            "dog --> pobj --> over\n",
            ". --> punct --> jumps\n",
            "\n",
            "Constituency Parsing with NLTK:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** For a given corpus, dependency parsing and constituency parsing has been implemented sucessfully."
      ],
      "metadata": {
        "id": "KFn_beZjc3nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Thus the given task to perform different operations has been executed successfully."
      ],
      "metadata": {
        "id": "JMzejzWWf425"
      }
    }
  ]
}